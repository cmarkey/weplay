x.ids <- c()
smp_size2 = floor(0.70 * nrow(x_train))
x_test2 = x_train[smp_size2,]
y_test2 = unlist(y_train)[smp_size2]
for(t in 1:length(try.iterations)){
for(e in 1:length(try.eta)){
for(b in 1:length(try.bag)){
x.boost <- xgboost(data=as.matrix(x_train), label=unlist(y_train),
eta=try.eta[e],
subsample=try.bag[b],
nrounds=try.iterations[t],
objective="reg:squarederror",
verbose = 0)
xest.mspe <- c(xest.mspe,sqrt(mean((unlist(y_test2)-predict(x.boost,newdata=as.matrix(x_test2)))^2)))
x.ids <- c(x.ids, paste(try.iterations[t], try.eta[e], try.bag[b], sep=" "))
}
}
}
chosen <- as.numeric(strsplit(x.ids[which.min(xest.mspe)], " ")[[1]]) # 57.000  0.175  0.500
print(chosen)
best.xtree.boost <- xgboost(data=as.matrix(x_train), label=unlist(y_train),
eta=chosen[2],
subsample=chosen[3],
nrounds=chosen[1],
objective="reg:squarederror",
verbose = 0)
TrainPreds <- predict(best.xtree.boost, newdata=as.matrix(x_train))
TestPreds <- predict(best.xtree.boost, newdata=as.matrix(x_test))
Preds <- predict(best.xtree.boost, newdata=as.matrix(ipl_w_avg[,1:2]))
MSE = sqrt(mean((unlist(y_train)-TrainPreds)^2))
MSPE = sqrt(mean((unlist(y_test)-TestPreds)^2))
print(paste("Mean Squared Error for training data",round(MSE,4),"and Mean Squared Prediction Error for test data", round(MSPE,4)))
sqrt(mean((ipl_w_avg$y-Preds)^2))
cbind(ipl_w_avg, Preds)
cbind(ipl_w_avg, predictions=Preds)
kable(cbind(ipl_w_avg, predictions=Preds), caption="Average expected runs earned observed vs. predicted for each wicket/overs remaining combo")
kable(arrange(ipl_w_avg,desc(overs_remaining),desc(wickets_remaining)),caption= "Average Run Earned by Wickets and Overs Remaining")
kable(cbind(ipl_w_avg, predictions=Preds), caption="Average expected runs earned observed vs. predicted for each wicket/overs remaining combo")
View(ipl_w_earned)
View(observed)
kable(cbind(ipl_w_avg, predictions=Preds), caption="Average expected runs earned observed vs. predicted for each wicket/overs remaining combo")
## TRAIN TEST SPLIT ##
# 75% of the sample size
smp_size = floor(0.70 * nrow(ipl_w_avg))
# set the seed to make your partition reproducible
set.seed(123)
train_ind = sample(seq_len(nrow(ipl_w_avg)), size = smp_size)
x_train = ipl_w_avg[train_ind, -3]
x_test = ipl_w_avg[-train_ind, -3]
y_train = ipl_w_avg[train_ind,3]
y_test =  ipl_w_avg[-train_ind,3]
print(paste("Train samples:", nrow(x_train), " Test samples:", nrow(x_test)))
set.seed(12345)
try.iterations <- c(54,56,58,60,62,64)
try.eta <- c(0.1, 0.125, 0.15, 0.175)
try.bag <- c(0.3, 0.4, 0.5)
xest.mspe <- c()
x.ids <- c()
smp_size2 = floor(0.70 * nrow(x_train))
x_test2 = x_train[smp_size2,]
y_test2 = unlist(y_train)[smp_size2]
for(t in 1:length(try.iterations)){
for(e in 1:length(try.eta)){
for(b in 1:length(try.bag)){
x.boost <- xgboost(data=as.matrix(x_train), label=unlist(y_train),
eta=try.eta[e],
subsample=try.bag[b],
nrounds=try.iterations[t],
objective="reg:squarederror",
verbose = 0)
xest.mspe <- c(xest.mspe,sqrt(mean((unlist(y_test2)-predict(x.boost,newdata=as.matrix(x_test2)))^2)))
x.ids <- c(x.ids, paste(try.iterations[t], try.eta[e], try.bag[b], sep=" "))
}
}
}
chosen <- as.numeric(strsplit(x.ids[which.min(xest.mspe)], " ")[[1]]) # 57.000  0.175  0.500
print(chosen)
best.xtree.boost <- xgboost(data=as.matrix(x_train), label=unlist(y_train),
eta=chosen[2],
subsample=chosen[3],
nrounds=chosen[1],
objective="reg:squarederror",
verbose = 0)
TrainPreds <- predict(best.xtree.boost, newdata=as.matrix(x_train))
TestPreds <- predict(best.xtree.boost, newdata=as.matrix(x_test))
Preds <- predict(best.xtree.boost, newdata=as.matrix(ipl_w_avg[,1:2]))
MSE = sqrt(mean((unlist(y_train)-TrainPreds)^2))
MSPE = sqrt(mean((unlist(y_test)-TestPreds)^2))
print(paste("Mean Squared Error for training data",round(MSE,4),"and Mean Squared Prediction Error for test data", round(MSPE,4)))
list.of.packages <- c("readr","rgl","ggplot2","knitr","rglwidget")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(list.of.packages,function(x){library(x,character.only=TRUE)})
knit_hooks$set(webgl = hook_webgl)
dataset <- read_csv("C:/Users/Paula/Downloads/dataset_draft.csv")
View(dataset)
library(RColorBrewer)
cols = brewer.pal(8, "Blues")
# Define colour pallete
pal = colorRampPalette(c("blue", "red"))
# Use the following line with RColorBrewer
pal = colorRampPalette(cols)
# Rank variable for colour assignment
dataset$vel = sqrt(dataset$`U:0`^2+dataset$`U:1`^2+dataset$`U:2`^2)
dataset$order = findInterval(dataset$p, sort(dataset$p))
plot3d(dataset$`Points:0`,0,dataset$`Points:2`, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
dataset$order = findInterval(dataset$omega, sort(dataset$omega))
plot3d(dataset$`Points:0`,0,dataset$`Points:2`, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
dataset$order = findInterval(dataset$k, sort(dataset$k))
plot3d(dataset$`Points:0`,0,dataset$`Points:2`, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
plot3d(dataset$`UNear:0`,0,dataset$`UNear:2`, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
dataset$order = findInterval(dataset$p, sort(dataset$p))
plot3d(dataset$`UNear:0`,0,dataset$`UNear:2`, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
dataset$order = findInterval(dataset$omega, sort(dataset$omega))
plot3d(dataset$`UNear:0`,0,dataset$`UNear:2`, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
plot3d(dataset$`U:0`,0,dataset$`U:2`, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
dataset$order = findInterval(dataset$nut, sort(dataset$nut))
plot3d(dataset$`Points:0`,0,dataset$`Points:2`, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
plot3d(0,dataset$`Points:1`,dataset$`Points:2`, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
plot3d(dataset$`Points:0`,0,dataset$`Points:2`, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
plot3d(dataset$`Points:0`,dataset$`Points:1`,0, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
plot3d(dataset$`Points:0`,dataset$`Points:1`,4, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
plot3d(dataset$`Points:0`,dataset$`Points:1`,3, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
plot3d(dataset$`Points:0`,dataset$`Points:1`,1.5, aspect = FALSE, col = pal(nrow(dataset))[dataset$order],xlim=c(-5,15),ylim=c(-4,4),zlim=c(0,8),
type="p", size=3, lit=FALSE, main = "Car Weight Vs Engine Displacement Vs Mileage",sub="3-D Plot")
p = [1 2 3 4 5 6]
p = array(c(1,2,3,4,5,6))
p*p
p*t(p)
t(p)*p
p = matrix(c(1,2,3,4,5,6))
p
p*t(p)
p%*%t(p)
p%-%t(p)
library(installr)
install.packages('installr')
library(installr)
updateR()
# Load (install if needed) the required packages
load.libraries = c("readr","tidyverse","dplyr","randomForest","xgboost","rlang","rminer","corrplot")
install.lib = load.libraries[!load.libraries %in% installed.packages()]
for (libs in install.lib) {install.packages(libs, dependencies = TRUE)}
sapply(load.libraries, require, character = TRUE)
all_data = read_csv("H:/Hockey/Current Git/Big-Data-Cup-2022-Private/data_all_metrics_Nov29_final.csv")
# y= assumed_danger_states_new
data_4_model0 = all_data[,c("assumed_danger_states_new","situation_type","goals_for","goals_against",
"Home_Plate_Control","Rink_Control","Max_Success","Max_Best","Max_Exp","Max_Player_Success","Max_Player_Best","Max_Player_Exp","Mean_Player_Success","Mean_Player_Best","Mean_Player_Exp",
"O Players","D Players",
"All_Avg_Edge","O_Avg_Edge","D_Avg_Edge","All_Avg_Edges per Player","O_Avg_Edges_per_Player","D_Avg_Edges per Player",
"OD_MST_Ratio","All_OCR",
"distance_to_net","angle_to_attacking_net")]
# y= assumed_danger_states_new
data_4_model0 = all_data[,c("assumed_danger_states_new","situation_type",
"Home_Plate_Control","Rink_Control","Max_Success","Max_Best","Max_Exp","Max_Player_Success","Max_Player_Best","Max_Player_Exp","Mean_Player_Success","Mean_Player_Best","Mean_Player_Exp",
"All_Avg_Edge","O_Avg_Edge","D_Avg_Edge","All_Avg_Edges per Player","O_Avg_Edges_per_Player","D_Avg_Edges per Player",
"OD_MST_Ratio","All_OCR",
"distance_to_net","angle_to_attacking_net")]
data_4_model0 = data_4_model0[!is.na(rowSums(data_4_model0[,7:ncol(data_4_model0)])),]
colnames(data_4_model0)
data_4_model0 = data_4_model0[!is.na(rowSums(data_4_model0[,3:ncol(data_4_model0)])),]
summary(data_4_model0)
data_4_model = data_4_model0 %>%
mutate(woman_adv = as.numeric(substr(situation_type,0,1))-as.numeric(substr(situation_type,6,7))) %>%
select(-c(situation_type))
summary(data_4_model)
#to make the correlation matrix plot
corrplot(cor(data_4_model0[,-c(2:4)]))
# y= assumed_danger_states_new
data_4_model0 = all_data[,c("assumed_danger_states_new","situation_type",
"Home_Plate_Control","Rink_Control","Max_Success","Max_Best","Max_Exp",#"Max_Player_Success","Max_Player_Best","Max_Player_Exp","Mean_Player_Success","Mean_Player_Best","Mean_Player_Exp",
"All_Avg_Edge","O_Avg_Edge","D_Avg_Edge","All_Avg_Edges per Player","O_Avg_Edges_per_Player","D_Avg_Edges per Player",
"OD_MST_Ratio","All_OCR",
"distance_to_net","angle_to_attacking_net")]
data_4_model0 = data_4_model0[!is.na(rowSums(data_4_model0[,3:ncol(data_4_model0)])),]
data_4_model = data_4_model0 %>%
mutate(woman_adv = as.numeric(substr(situation_type,0,1))-as.numeric(substr(situation_type,6,7))) %>%
select(-c(situation_type))
#to make the correlation matrix plot
corrplot(cor(data_4_model0[,-c(2:4)]))
set.seed(1)
data_4_model$split = 0
data_4_model$split[which(data_4_model$assumed_danger_states_new==0)] = ifelse(runif(n=length(which(data_4_model$assumed_danger_states_new==0)))>0.7, yes=1, no=0)
data_4_model$split[which(data_4_model$assumed_danger_states_new==1)] = ifelse(runif(n=length(which(data_4_model$assumed_danger_states_new==1)))>0.7, yes=1, no=0)
lm_model = lm(assumed_danger_states_new~.,data=data_4_model[data_4_model$split==0,])
lm_model1 = lm(assumed_danger_states_new~1,data=data_4_model[data_4_model$split==0,])
step = step(lm_model1, scope=list(upper = lm_model,lower= lm_model1),direction="both", trace=0)
mean(as.numeric(predict(step,newdata=data_4_model[data_4_model$split==1,])>0.5)!=y_test) #0.2523364
y_train = data_4_model1$assumed_danger_states_new[data_4_model1$split==0] %>% as.factor()
y_test = data_4_model1$assumed_danger_states_new[data_4_model1$split==1] %>% as.factor()
x_train = data_4_model1[data_4_model1$split==0,-c(1,ncol(data_4_model1))]
y_train = data_4_model$assumed_danger_states_new[data_4_model$split==0] %>% as.factor()
y_test = data_4_model$assumed_danger_states_new[data_4_model$split==1] %>% as.factor()
x_train = data_4_model[data_4_model$split==0,-c(1,ncol(data_4_model))]
x_test = data_4_model[data_4_model$split==1,-c(1,ncol(data_4_model))]
lm_model = lm(assumed_danger_states_new~.,data=data_4_model[data_4_model$split==0,])
lm_model1 = lm(assumed_danger_states_new~1,data=data_4_model[data_4_model$split==0,])
step = step(lm_model1, scope=list(upper = lm_model,lower= lm_model1),direction="both", trace=0)
mean(as.numeric(predict(step,newdata=data_4_model[data_4_model$split==1,])>0.5)!=y_test) #0.2523364
mean(as.numeric(0!=y_test) #0.2523364
)
set.seed(1)
data_4_model$split = 0
data_4_model$split[which(data_4_model$assumed_danger_states_new==0)] = ifelse(runif(n=length(which(data_4_model$assumed_danger_states_new==0)))>0.7, yes=1, no=0)
data_4_model$split[which(data_4_model$assumed_danger_states_new==1)] = ifelse(runif(n=length(which(data_4_model$assumed_danger_states_new==1)))>0.7, yes=1, no=0)
y_train = data_4_model$assumed_danger_states_new[data_4_model$split==0] %>% as.factor()
y_test = data_4_model$assumed_danger_states_new[data_4_model$split==1] %>% as.factor()
x_train = data_4_model[data_4_model$split==0,-c(1,ncol(data_4_model))]
x_test = data_4_model[data_4_model$split==1,-c(1,ncol(data_4_model))]
lm_model = lm(assumed_danger_states_new~.,data=data_4_model[data_4_model$split==0,])
lm_model1 = lm(assumed_danger_states_new~1,data=data_4_model[data_4_model$split==0,])
step = step(lm_model1, scope=list(upper = lm_model,lower= lm_model1),direction="both", trace=0)
mean(as.numeric(predict(step,newdata=data_4_model[data_4_model$split==1,])>0.5)!=y_test) #0.228972
mean(as.numeric(0!=y_test)) #0.2336449
step
#Random Forest
p = ncol(x_train)
rf1 <- randomForest(x=x_train, y=y_train, importance=TRUE, ntree=2500,
mtry=1, keep.forest=TRUE)
rf1$confusion
varImpPlot(rf1)
test_preds_rf1 = predict(rf1,newdata=x_test)
conf_rf1 = mmetric(y_test,test_preds_rf1,metric="CONF")$conf
#sensitivity
conf_rf1[1,1]/sum(conf_rf1[1,])
#precision
conf_rf1[1,1]/sum(conf_rf1[,1])
#specificity
conf_rf1[2,2]/sum(conf_rf1[2,])
#accuracy
(conf_rf1[1,1]+conf_rf1[2,2])/sum(conf_rf1)
rf2 <- randomForest(x=x_train, y=y_train, importance=TRUE, ntree=2500,
mtry=round(p/5), keep.forest=TRUE)
rf2$confusion
varImpPlot(rf2)
test_preds_rf2 = predict(rf2,newdata=x_test)
conf_rf2 = mmetric(y_test,test_preds_rf2,metric="CONF")$conf
#sensitivity
conf_rf2[1,1]/sum(conf_rf2[1,])
#precision
conf_rf2[1,1]/sum(conf_rf2[,1])
#specificity
conf_rf2[2,2]/sum(conf_rf2[2,])
#accuracy
(conf_rf2[1,1]+conf_rf2[2,2])/sum(conf_rf2)
rf3 <- randomForest(x=x_train, y=y_train, importance=TRUE, ntree=2500,
mtry=round(p/4), keep.forest=TRUE)
rf3$confusion
varImpPlot(rf3)
test_preds_rf3 = predict(rf3,newdata=x_test)
conf_rf3 = mmetric(y_test,test_preds_rf3,metric="CONF")$conf
#sensitivity
conf_rf3[1,1]/sum(conf_rf3[1,])
#precision
conf_rf3[1,1]/sum(conf_rf3[,1])
#specificity
conf_rf3[2,2]/sum(conf_rf3[2,])
#accuracy
(conf_rf3[1,1]+conf_rf3[2,2])/sum(conf_rf3)
rf4 <- randomForest(x=x_train, y=y_train, importance=TRUE, ntree=2500,
mtry=round(p/3), keep.forest=TRUE)
rf4$confusion
varImpPlot(rf4)
test_preds_rf4 = predict(rf4,newdata=x_test)
conf_rf4 = mmetric(y_test,test_preds_rf4,metric="CONF")$conf
#sensitivity
conf_rf4[1,1]/sum(conf_rf4[1,])
#precision
conf_rf4[1,1]/sum(conf_rf4[,1])
#specificity
conf_rf4[2,2]/sum(conf_rf4[2,])
#accuracy
(conf_rf4[1,1]+conf_rf4[2,2])/sum(conf_rf4)
rf5 <- randomForest(x=x_train, y=y_train, importance=TRUE, ntree=2500,
mtry=round(p/2), keep.forest=TRUE)
rf5$confusion
varImpPlot(rf5)
test_preds_rf5 = predict(rf5,newdata=x_test)
conf_rf5 = mmetric(y_test,test_preds_rf5,metric="CONF")$conf
conf_rf5
#sensitivity
conf_rf5[1,1]/sum(conf_rf5[1,])
#precision
conf_rf5[1,1]/sum(conf_rf5[,1])
#specificity
conf_rf5[2,2]/sum(conf_rf5[2,])
#accuracy
(conf_rf5[1,1]+conf_rf5[2,2])/sum(conf_rf5)
#test error
mean(test_preds_rf1 != y_test)
mean(test_preds_rf2 != y_test)
mean(test_preds_rf3 != y_test) #step 0.1962617
mean(test_preds_rf4 != y_test)
mean(test_preds_rf5 != y_test) #all vars 0.2102804
x_train_dummies = x_train
y_train_dummies = as.numeric(y_train)-1
x_test_dummies = x_test
y_test_dummies = as.numeric(y_test)-1
data_train = list(x_train=as.matrix(x_train_dummies),y_train=y_train_dummies)
dtrain <- xgb.DMatrix(data = data_train$x_train, label = data_train$y_train)
data_test = list(x_test=as.matrix(x_test_dummies),y_test=y_test_dummies)
dtest <- xgb.DMatrix(data = data_test$x_test, label = data_test$y_test)
set.seed(1)
try.iterations <- c(10,15,20,25,30,35,40,45,50, 55)
try.eta <- c(0.18, 0.2,0.22,0.25,0.28,0.3,0.32,0.35)
try.bag <- c(0.6, 0.65, 0.7, 0.75, 0.8)
xest.mspe <- c()
x.ids <- c()
for(t in 1:length(try.iterations)){
#for(d in 1:length(try.depth)){
for(e in 1:length(try.eta)){
for(b in 1:length(try.bag)){
x.boost <- xgb.train(data = dtrain,
#max_depth=try.depth[d],
eta=try.eta[e],
subsample=try.bag[b],
nrounds=try.iterations[t],
objective="binary:logistic",
verbose = 0,
eval_metric='logloss')
xest.mspe <- c(xest.mspe,mean(as.numeric(predict(x.boost, data_train$x_train) > 0.5) != data_train$y_train))
x.ids <- c(x.ids, paste(try.iterations[t], try.eta[e], try.bag[b], sep=" "))
}
}
#}
}
chosen <- as.numeric(strsplit(x.ids[which.min(xest.mspe)], " ")[[1]])
x.boost <- xgb.train(data = dtrain,
#max_depth=try.depth[d],
eta=chosen[2],
subsample=chosen[3],
nrounds=chosen[1],
objective="binary:logistic",
verbose = 0,
eval_metric='logloss')
pred <- predict(x.boost, data_test$x_test)
prediction <- as.numeric(pred > 0.5)
conf_xgb_best = mmetric(y_test,prediction,metric="CONF")$conf
conf_xgb_best
#sensitivity
conf_xgb_best[1,1]/sum(conf_xgb_best[1,])
#precision
conf_xgb_best[1,1]/sum(conf_xgb_best[,1])
#specificity
conf_xgb_best[2,2]/sum(conf_xgb_best[2,])
#accuracy
(conf_xgb_best[1,1]+conf_xgb_best[2,2])/sum(conf_xgb_best)
#test error
mean(prediction != data_test$y_test) #step = 0.1775701, all_vars=0.2056075
#to beat
mean(y_test==1) #0.2336449
data_4_model1 = data_4_model %>% select(c(assumed_danger_states_new,distance_to_net, Rink_Control,
All_OCR, Home_Plate_Control, angle_to_attacking_net, woman_adv,split))
y_train = data_4_model1$assumed_danger_states_new[data_4_model1$split==0] %>% as.factor()
y_test = data_4_model1$assumed_danger_states_new[data_4_model1$split==1] %>% as.factor()
x_train = data_4_model1[data_4_model1$split==0,-c(1,ncol(data_4_model1))]
x_test = data_4_model1[data_4_model1$split==1,-c(1,ncol(data_4_model1))]
p = ncol(x_train)
rf1 <- randomForest(x=x_train, y=y_train, importance=TRUE, ntree=2500,
mtry=1, keep.forest=TRUE)
rf1$confusion
varImpPlot(rf1)
test_preds_rf1 = predict(rf1,newdata=x_test)
conf_rf1 = mmetric(y_test,test_preds_rf1,metric="CONF")$conf
#sensitivity
conf_rf1[1,1]/sum(conf_rf1[1,])
#precision
conf_rf1[1,1]/sum(conf_rf1[,1])
#specificity
conf_rf1[2,2]/sum(conf_rf1[2,])
#accuracy
(conf_rf1[1,1]+conf_rf1[2,2])/sum(conf_rf1)
rf2 <- randomForest(x=x_train, y=y_train, importance=TRUE, ntree=2500,
mtry=round(p/5), keep.forest=TRUE)
rf2$confusion
varImpPlot(rf2)
test_preds_rf2 = predict(rf2,newdata=x_test)
conf_rf2 = mmetric(y_test,test_preds_rf2,metric="CONF")$conf
#sensitivity
conf_rf2[1,1]/sum(conf_rf2[1,])
#precision
conf_rf2[1,1]/sum(conf_rf2[,1])
#specificity
conf_rf2[2,2]/sum(conf_rf2[2,])
#accuracy
(conf_rf2[1,1]+conf_rf2[2,2])/sum(conf_rf2)
rf3 <- randomForest(x=x_train, y=y_train, importance=TRUE, ntree=2500,
mtry=round(p/4), keep.forest=TRUE)
rf3$confusion
varImpPlot(rf3)
test_preds_rf3 = predict(rf3,newdata=x_test)
conf_rf3 = mmetric(y_test,test_preds_rf3,metric="CONF")$conf
#sensitivity
conf_rf3[1,1]/sum(conf_rf3[1,])
#precision
conf_rf3[1,1]/sum(conf_rf3[,1])
#specificity
conf_rf3[2,2]/sum(conf_rf3[2,])
#accuracy
(conf_rf3[1,1]+conf_rf3[2,2])/sum(conf_rf3)
rf4 <- randomForest(x=x_train, y=y_train, importance=TRUE, ntree=2500,
mtry=round(p/3), keep.forest=TRUE)
rf4$confusion
varImpPlot(rf4)
test_preds_rf4 = predict(rf4,newdata=x_test)
conf_rf4 = mmetric(y_test,test_preds_rf4,metric="CONF")$conf
#sensitivity
conf_rf4[1,1]/sum(conf_rf4[1,])
#precision
conf_rf4[1,1]/sum(conf_rf4[,1])
#specificity
conf_rf4[2,2]/sum(conf_rf4[2,])
#accuracy
(conf_rf4[1,1]+conf_rf4[2,2])/sum(conf_rf4)
rf5 <- randomForest(x=x_train, y=y_train, importance=TRUE, ntree=2500,
mtry=round(p/2), keep.forest=TRUE)
rf5$confusion
varImpPlot(rf5)
test_preds_rf5 = predict(rf5,newdata=x_test)
conf_rf5 = mmetric(y_test,test_preds_rf5,metric="CONF")$conf
conf_rf5
#sensitivity
conf_rf5[1,1]/sum(conf_rf5[1,])
#precision
conf_rf5[1,1]/sum(conf_rf5[,1])
#specificity
conf_rf5[2,2]/sum(conf_rf5[2,])
#accuracy
(conf_rf5[1,1]+conf_rf5[2,2])/sum(conf_rf5)
#test error
mean(test_preds_rf1 != y_test)
mean(test_preds_rf2 != y_test)
mean(test_preds_rf3 != y_test) #step 0.1962617
mean(test_preds_rf4 != y_test) #all vars 0.1962617
mean(test_preds_rf5 != y_test)
x_train_dummies = x_train
y_train_dummies = as.numeric(y_train)-1
x_test_dummies = x_test
y_test_dummies = as.numeric(y_test)-1
data_train = list(x_train=as.matrix(x_train_dummies),y_train=y_train_dummies)
dtrain <- xgb.DMatrix(data = data_train$x_train, label = data_train$y_train)
data_test = list(x_test=as.matrix(x_test_dummies),y_test=y_test_dummies)
dtest <- xgb.DMatrix(data = data_test$x_test, label = data_test$y_test)
set.seed(1)
try.iterations <- c(10,15,20,25,30,35,40,45,50, 55)
try.eta <- c(0.18, 0.2,0.22,0.25,0.28,0.3,0.32,0.35)
try.bag <- c(0.6, 0.65, 0.7, 0.75, 0.8)
xest.mspe <- c()
x.ids <- c()
for(t in 1:length(try.iterations)){
#for(d in 1:length(try.depth)){
for(e in 1:length(try.eta)){
for(b in 1:length(try.bag)){
x.boost <- xgb.train(data = dtrain,
#max_depth=try.depth[d],
eta=try.eta[e],
subsample=try.bag[b],
nrounds=try.iterations[t],
objective="binary:logistic",
verbose = 0,
eval_metric='logloss')
xest.mspe <- c(xest.mspe,mean(as.numeric(predict(x.boost, data_train$x_train) > 0.5) != data_train$y_train))
x.ids <- c(x.ids, paste(try.iterations[t], try.eta[e], try.bag[b], sep=" "))
}
}
#}
}
chosen <- as.numeric(strsplit(x.ids[which.min(xest.mspe)], " ")[[1]])
x.boost <- xgb.train(data = dtrain,
#max_depth=try.depth[d],
eta=chosen[2],
subsample=chosen[3],
nrounds=chosen[1],
objective="binary:logistic",
verbose = 0,
eval_metric='logloss')
pred <- predict(x.boost, data_test$x_test)
prediction <- as.numeric(pred > 0.5)
conf_xgb_best = mmetric(y_test,prediction,metric="CONF")$conf
conf_xgb_best
#sensitivity
conf_xgb_best[1,1]/sum(conf_xgb_best[1,])
#precision
conf_xgb_best[1,1]/sum(conf_xgb_best[,1])
#specificity
conf_xgb_best[2,2]/sum(conf_xgb_best[2,])
#accuracy
(conf_xgb_best[1,1]+conf_xgb_best[2,2])/sum(conf_xgb_best)
#test error
mean(prediction != data_test$y_test) #step = 0.1775701, all_vars=0.1869159
#to beat
mean(y_test==1) #0.2336449
#all vars
write_csv(data_4_model,file='data_2_model.csv')
setwd("~/")
setwd("C:/Users/Paula/Desktop/weplay/Modelling Code")
#all vars
write_csv(data_4_model,file='data_2_model.csv')
step
